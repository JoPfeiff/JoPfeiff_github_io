---
---

@article{Pfeiffer2020unks,
selected={true},
abstract = {Massively multilingual language models such as multilingual BERT (mBERT) and XLM-R offer state-of-the-art cross-lingual transfer performance on a range of NLP tasks. However, due to their limited capacity and large differences in pretraining data, there is a profound performance gap between resource-rich and resource-poor target languages. The ultimate challenge is dealing with under-resourced languages not covered at all by the models, which are also written in scripts \textit{unseen} during pretraining. In this work, we propose a series of novel data-efficient methods that enable quick and effective adaptation of pretrained multilingual models to such low-resource languages and unseen scripts. Relying on matrix factorization, our proposed methods capitalize on the existing latent knowledge about multiple languages already available in the pretrained model's embedding matrix. Furthermore, we show that learning of the new dedicated embedding matrix in the target language can be improved by leveraging a small number of vocabulary items (i.e., the so-called \textit{lexically overlapping} tokens) shared between mBERT's and target language vocabulary. Our adaptation techniques offer substantial performance gains for languages with unseen scripts. We also demonstrate that they can also yield improvements for low-resource languages written in scripts covered by the pretrained model.},
author = {Jonas Pfeiffer and Vuli{\'{c}}, Ivan and Gurevych, Iryna and Ruder, Sebastian},
journal = {arXiv preprint},
title = {{UNKs Everywhere: Adapting Multilingual Language Models to New Scripts}},
pdf = {https://arxiv.org/pdf/2012.15562.pdf},
year = {2021}
}

@article{DBLP:journals/corr/abs-2104-08247,
  author    = {Clifton Poth and
               Jonas Pfeiffer and
               Andreas R{\"{u}}ckl{\'{e}} and
               Iryna Gurevych},
  title     = {{What to Pre-Train on? Efficient Intermediate Task Selection}},
  journal   = {arXiv preprint},
  year      = {2021},
  pdf       = {https://arxiv.org/abs/2104.08247},
  archivePrefix = {arXiv},
  eprint    = {2104.08247},
  timestamp = {Mon, 19 Apr 2021 16:45:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08247.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {{Intermediate task fine-tuning has been shown to culminate in large transfer gains across many NLP tasks. With an abundance of candidate datasets as well as pre-trained language models, it has become infeasible to run the cross-product of all combinations to find the best transfer setting. In this work we first establish that similar sequential fine-tuning gains can be achieved in adapter settings, and subsequently consolidate previously proposed methods that efficiently identify beneficial tasks for intermediate transfer learning. We experiment with a diverse set of 42 intermediate and 11 target English classification, multiple choice, question answering, and sequence tagging tasks. Our results show that efficient embedding based methods that rely solely on the respective datasets outperform computational expensive few-shot fine-tuning approaches. Our best methods achieve an average Regret@3 of less than 1% across all target tasks, demonstrating that we are able to efficiently identify the best datasets for intermediate training.}}
}


@article{DBLP:journals/corr/abs-2103-11920,
  author    = {Gregor Geigle and
               Jonas Pfeiffer and
               Nils Reimers and
               Ivan Vuli\'c and
               Iryna Gurevych},
  title     = {{Retrieve Fast, Rerank Smart: Cooperative and Joint Approaches for
               Improved Cross-Modal Retrieval}},
  journal   = {arXiv preprint},
  year      = {2021},
  pdf       = {https://arxiv.org/abs/2103.11920},
  archivePrefix = {arXiv},
  eprint    = {2103.11920},
  timestamp = {Wed, 24 Mar 2021 15:50:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-11920.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {{Current state-of-the-art approaches to cross-modal retrieval process text and visual input jointly, relying on Transformer-based architectures with cross-attention mechanisms that attend over all words and objects in an image. While offering unmatched retrieval performance, such models: 1) are typically pretrained from scratch and thus less scalable, 2) suffer from huge retrieval latency and inefficiency issues, which makes them impractical in realistic applications. To address these crucial gaps towards both improved and efficient cross-modal retrieval, we propose a novel fine-tuning framework which turns any pretrained text-image multi-modal model into an efficient retrieval model. The framework is based on a cooperative retrieve-and-rerank approach which combines: 1) twin networks to separately encode all items of a corpus, enabling efficient initial retrieval, and 2) a cross-encoder component for a more nuanced (i.e., smarter) ranking of the retrieved small set of items. We also propose to jointly fine-tune the two components with shared weights, yielding a more parameter-efficient model. Our experiments on a series of standard cross-modal retrieval benchmarks in monolingual, multilingual, and zero-shot setups, demonstrate improved accuracy and huge efficiency benefits over the state-of-the-art cross-encoders.}}
}



@inproceedings{Rust2021tokenizer,
      title     = {{How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models}}, 
      author    = {Phillip Rust and Jonas Pfeiffer and Ivan Vuli{\'c} and Sebastian Ruder and Iryna Gurevych},
      year      = {2021},
      booktitle = {Proceedings of ACL},
      pdf       = {https://arxiv.org/pdf/2012.15613.pdf},
    publisher = "Association for Computational Linguistics",
    abstract = {{In this work we provide a \textit{systematic empirical comparison} of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first establish if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for a performance difference. To disentangle the impacting variables, we train new monolingual models on the same data, but with different tokenizers, both the monolingual and the multilingual version. We find that while the pretraining data size is an important factor, the designated tokenizer of the monolingual model plays an equally important role in the downstream performance. Our results show that languages which are adequately represented in the multilingual model's vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language. }}
}


@inproceedings{pfeiffer-etal-2021-adapterfusion,
    selected={true},
    title = "{A}dapter{F}usion: {Non-Destructive Task Composition for Transfer Learning}",
    author = {Jonas Pfeiffer  and
      Kamath, Aishwarya  and
      R{\"u}ckl{\'e}, Andreas  and
      Cho, Kyunghyun  and
      Gurevych, Iryna},
    booktitle = "Proceedings of EACL",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pdf = "https://www.aclweb.org/anthology/2021.eacl-main.39.pdf",
    pages = "487--503",
    abstract = "Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.",
}




















@inproceedings{DBLP:conf/emnlp/PfeifferVGR20,
  author    = {Jonas Pfeiffer and
               Ivan Vuli\'c and
               Iryna Gurevych and
               Sebastian Ruder},
  selected={true},
  title     = {{MAD-X:} {An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer}},
  booktitle = {Proceedings of EMNLP},
  pages     = {7654--7673},
  year      = {2020},
  pdf       = {https://doi.org/10.18653/v1/2020.emnlp-main.617},
  doi       = {10.18653/v1/2020.emnlp-main.617},
  timestamp = {Fri, 08 Jan 2021 21:21:04 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/PfeifferVGR20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {{The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross-lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml}}
}



@inproceedings{DBLP:conf/emnlp/PfeifferRPKVRCG20,
  author    = {Jonas Pfeiffer  and
               Andreas R{\"{u}}ckl{\'{e}} and
               Clifton Poth and
               Aishwarya Kamath and
               Ivan Vuli\'c and
               Sebastian Ruder and
               Kyunghyun Cho and
               Iryna Gurevych},
  selected={true},
  title     = {{AdapterHub: A Framework for Adapting Transformers}},
  booktitle = {Proceedings of EMNLP - System Demonstrations},
  pages     = {46--54},
  year      = {2020},
  pdf       = {https://doi.org/10.18653/v1/2020.emnlp-demos.7},
  doi       = {10.18653/v1/2020.emnlp-demos.7},
  timestamp = {Fri, 08 Jan 2021 21:20:48 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/PfeifferRPKVRCG20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {{The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters -- small learnt bottleneck layers inserted within each layer of a pre-trained model -- ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic "stitching-in" of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at https://AdapterHub.ml.}}
}




@inproceedings{DBLP:conf/emnlp/RucklePG20,
  author    = {Andreas R{\"{u}}ckl{\'{e}} and
               Jonas Pfeiffer and
               Iryna Gurevych},
  title     = {{MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models
               on a Massive Scale}},
  booktitle = {Proceedings of EMNLP},
  pages     = {2471--2486},
  year      = {2020},
  pdf       = {https://doi.org/10.18653/v1/2020.emnlp-main.194},
  doi       = {10.18653/v1/2020.emnlp-main.194},
  timestamp = {Fri, 08 Jan 2021 21:21:06 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/RucklePG20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {{We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English. We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines. We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains. In addition, we extensively study how to best combine multiple source domains. We propose to incorporate self-supervised with supervised multi-task learning on all available source domains. Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks. Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.}}
}

@article{DBLP:journals/corr/abs-2010-11918,
  author    = {Andreas R{\"{u}}ckl{\'{e}} and
               Gregor Geigle and
               Max Glockner and
               Tilman Beck and
               Jonas Pfeiffer and
               Nils Reimers and
               Iryna Gurevych},
  title     = {{AdapterDrop: On the Efficiency of Adapters in Transformers}},
  journal   = {arXiv preprint},
  year      = {2020},
  pdf       = {https://arxiv.org/abs/2010.11918},
  archivePrefix = {arXiv},
  eprint    = {2010.11918},
  timestamp = {Thu, 26 Nov 2020 12:13:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-11918.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {{Massively pre-trained transformer models are computationally expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from AdapterFusion, which improves the inference efficiency while maintaining the task performances entirely.}}
}


@inproceedings{DBLP:conf/aaai/SimpsonPG20,
  author    = {Edwin Simpson and
               Jonas Pfeiffer and
               Iryna Gurevych},
  title     = {{Low Resource Sequence Tagging with Weak Labels}},
  booktitle = {Proceedings of AAAI},
  pages     = {8862--8869},
  year      = {2020},
  pdf       = {https://aaai.org/ojs/index.php/AAAI/article/view/6415},
  timestamp = {Tue, 02 Feb 2021 08:00:27 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/SimpsonPG20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {{Current methods for sequence tagging depend on large quantities of domain-specific training data, limiting their use in new, user-defined tasks with few or no annotations. While crowdsourcing can be a cheap source of labels, it often introduces errors that degrade the performance of models trained on such crowdsourced data. Another solution is to use transfer learning to tackle low resource sequence labelling, but current approaches rely heavily on similar high resource datasets in different languages. In this paper, we propose a domain adaptation method using Bayesian sequence combination to exploit pre-trained models and unreliable crowdsourced data that does not require high resource data in a different language. Our method boosts performance by learning the relationship between each labeller and the target task and trains a sequence labeller on the target domain with little or no gold-standard data. We apply our approach to labelling diagnostic classes in medical and educational case studies, showing that the model achieves strong performance though zero-shot transfer learning and is more effective than alternative ensemble methods. Using NER and information extraction tasks, we show how our approach can train a model directly from crowdsourced labels, outperforming pipeline approaches that first aggregate the crowdsourced data, then train on the aggregated labels.}}
}


@article{DBLP:journals/corr/abs-2005-00250,
  author    = {Jonas Pfeiffer and
               Edwin Simpson and
               Iryna Gurevych},
  title     = {{Low Resource Multi-Task Sequence Tagging - Revisiting Dynamic Conditional
               Random Fields}},
  journal   = {arXiv preprint},
  year      = {2020},
  pdf       = {https://arxiv.org/abs/2005.00250},
  archivePrefix = {arXiv},
  eprint    = {2005.00250},
  timestamp = {Fri, 08 May 2020 15:04:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-00250.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {{We compare different models for low resource multi-task sequence tagging that leverage dependencies between label sequences for different tasks. Our analysis is aimed at datasets where each example has labels for multiple tasks. Current approaches use either a separate model for each task or standard multi-task learning to learn shared feature representations. However, these approaches ignore correlations between label sequences, which can provide important information in settings with small training datasets. To analyze which scenarios can profit from modeling dependencies between labels in different tasks, we revisit dynamic conditional random fields (CRFs) and combine them with deep neural networks. We compare single-task, multi-task and dynamic CRF setups for three diverse datasets at both sentence and document levels in English and German low resource scenarios. We show that including silver labels from pretrained part-of-speech taggers as auxiliary tasks can improve performance on downstream tasks. We find that especially in low-resource scenarios, the explicit modeling of inter-dependencies between task predictions outperforms single-task as well as standard multi-task models.}}
}





@inproceedings{DBLP:conf/rep4nlp/KamathPPGV19,
  author    = {Aishwarya Kamath and
               Jonas Pfeiffer and
               Edoardo Maria Ponti and
               Goran Glavaš and
               Ivan Vuli\'c},
  title     = {{Specializing Distributional Vectors of All Words for Lexical Entailment}},
  booktitle = {Proceedings of
               RepL4NLP@ACL 2019},
  pages     = {72--83},
  year      = {2019},
  pdf       = {https://doi.org/10.18653/v1/w19-4310},
  doi       = {10.18653/v1/w19-4310},
  timestamp = {Mon, 01 Mar 2021 13:46:58 +0100},
  biburl    = {https://dblp.org/rec/conf/rep4nlp/KamathPPGV19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {{Semantic specialization methods fine-tune distributional word vectors using lexical knowledge from external resources (e.g., WordNet) to accentuate a particular relation between words. However, such post-processing methods suffer from limited coverage as they affect only vectors of words seen in the external resources. We present the first postprocessing method that specializes vectors of all vocabulary words – including those unseen in the resources – for the asymmetric relation of lexical entailment (LE) (i.e., hyponymyhypernymy relation). Leveraging a partially LE-specialized distributional space, our POSTLE (i.e., post-specialization for LE) model learns an explicit global specialization function, allowing for specialization of vectors of unseen words, as well as word vectors from other languages via cross-lingual transfer. We capture the function as a deep feedforward neural network: its objective re-scales vector norms to reflect the concept hierarchy while simultaneously attracting hyponymyhypernymy pairs to better reflect semantic similarity. An extended model variant augments the basic architecture with an adversarial discriminator. We demonstrate the usefulness and versatility of POSTLE models with different input distributional spaces in different scenarios (monolingual LE and zero-shot cross-lingual LE transfer) and tasks (binary and graded LE). We report consistent gains over state-of-the-art LE-specialization methods, and successfully LE-specialize word vectors for languages without any external lexical knowledge.}}
}

@article{DBLP:journals/corr/abs-1909-04547,
  author    = {Jonas Pfeiffer and
               Aishwarya Kamath and
               Iryna Gurevych and
               Sebastian Ruder},
  title     = {{What do Deep Networks Like to Read?}},
  journal   = {arXiv preprint},
  year      = {2019},
  pdf       = {http://arxiv.org/abs/1909.04547},
  archivePrefix = {arXiv},
  eprint    = {1909.04547},
  timestamp = {Tue, 17 Sep 2019 11:23:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-04547.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {{Recent research towards understanding neural networks probes models in a top-down manner, but is only able to identify model tendencies that are known a priori. We propose Susceptibility Identification through Fine-Tuning (SIFT), a novel abstractive method that uncovers a model's preferences without imposing any prior. By fine-tuning an autoencoder with the gradients from a fixed classifier, we are able to extract propensities that characterize different kinds of classifiers in a bottom-up manner. We further leverage the SIFT architecture to rephrase sentences in order to predict the opposing class of the ground truth label, uncovering potential artifacts encoded in the fixed classification model. We evaluate our method on three diverse tasks with four different models. We contrast the propensities of the models as well as reproduce artifacts reported in the literature.}}
}

@inproceedings{DBLP:conf/emnlp/PfeifferMSKZSBF19,
  author    = {Jonas Pfeiffer and
               Christian M. Meyer and
               Claudia Schulz and
               Jan Kiesewetter and
               Jan M. Zottmann and
               Michael Sailer and
               Elisabeth Bauer and
               Frank Fischer and
               Martin R. Fischer and
               Iryna Gurevych},
  title     = {{FAMULUS: Interactive Annotation and Feedback Generation for Teaching
               Diagnostic Reasoning}},
  booktitle = {Proceedings of EMNLP - System Demonstrations},
  pages     = {73--78},
  year      = {2019},
  pdf       = {https://doi.org/10.18653/v1/D19-3013},
  doi       = {10.18653/v1/D19-3013},
  timestamp = {Fri, 13 Dec 2019 13:04:07 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/PfeifferMSKZSBF19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {{Our proposed system FAMULUS helps students learn to diagnose based on automatic feedback in virtual patient simulations, and it supports instructors in labeling training data. Diagnosing is an exceptionally difficult skill to obtain but vital for many different professions (e.g., medical doctors, teachers). Previous case simulation systems are limited to multiple-choice questions and thus cannot give constructive individualized feedback on a student's diagnostic reasoning process. Given initially only limited data, we leverage a (replaceable) NLP model to both support experts in their further data annotation with automatic suggestions, and we provide automatic feedback for students. We argue that because the central model consistently improves, our interactive approach encourages both students and instructors to recurrently use the tool, and thus accelerate the speed of data creation and annotation. We show results from two user studies on diagnostic reasoning in medicine and teacher education and outline how our system can be extended to further use cases.}}
}



@inproceedings{DBLP:conf/bionlp/PfeifferBGG18,
  author    = {Tariq Alhindi and
               Jonas Pfeiffer and
               Smaranda Muresan},
  title     = {{Fine-Tuned Neural Models for Propaganda Detection at the Sentence
               and Fragment levels}},
  booktitle = {Proceedings of NLP4IF@EMNLP},
  pages     = {98--102},
  year      = {2019},
  pdf = {http://arxiv.org/abs/1910.09702},
  abstract = {{This paper presents the CUNLP submission for the NLP4IF 2019 shared-task on FineGrained Propaganda Detection. Our system finished 5th out of 26 teams on the sentence-level classification task and 5th out of 11 teams on the fragment-level classification task based on our scores on the blind test set. We present our models, a discussion of our ablation studies and experiments, and an analysis of our performance on all eighteen propaganda techniques present in the corpus of the shared task.}}
}














@inproceedings{DBLP:conf/bionlp/PfeifferBGG18,
  author    = {Jonas Pfeiffer and
               Samuel Broscheit and
               Rainer Gemulla and
               Mathias G{\"{o}}schl},
  title     = {{A Neural Autoencoder Approach for Document Ranking and Query Refinement
               in Pharmacogenomic Information Retrieval}},
  booktitle = {Proceedings of BioNLP@ACL},
  pages     = {87--97},
  year      = {2018},
  pdf       = {https://doi.org/10.18653/v1/w18-2310},
  doi       = {10.18653/v1/w18-2310},
  timestamp = {Tue, 28 Jan 2020 10:29:22 +0100},
  biburl    = {https://dblp.org/rec/conf/bionlp/PfeifferBGG18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
